{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57597bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Large Language Models\n",
    "* **Created by:** Eric Martinez\n",
    "* **For:** 3351 - AI-Powered Applications\n",
    "* **At:** University of Texas Rio-Grande Valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "596fd155",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239d0ef",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae877e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U --quiet openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Create an OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eab05f7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=os.environ.get(\"OPENAI_API_BASE\"),\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def completion(prompt, max_tokens=None, temperature=0):\n",
    "    completion = client.completions.create(\n",
    "      model=\"gpt-3.5-turbo-instruct\",\n",
    "      prompt=prompt,\n",
    "      max_tokens=max_tokens,\n",
    "      temperature=temperature\n",
    "    )\n",
    "    return completion.choices[0].text.strip()\n",
    "\n",
    "def chat_completion(message, model=\"gpt-3.5-turbo\", prompt=None, temperature=0):\n",
    "    # Initialize the messages list\n",
    "    messages = []\n",
    "    \n",
    "    # Add the prompt to the messages list\n",
    "    if prompt is not None:\n",
    "        messages += [{\"role\": \"system\", \"content\": prompt}]\n",
    "    \n",
    "    # Add the user's message to the messages list\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796ac30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview of LLMs and their capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a715edc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An LLM is a machine learning model designed to understand and generate human-like text. They are trained on vast amounts of text data and have been found to be able to perform a wide range of tasks, such as translation, summarization, and question-answering.\n",
    "\n",
    "We don't quite know the full extent of the capabilities or limitations of large language models.\n",
    "\n",
    "Everyday new discoveries are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e416a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's explore 'completion' models\n",
    "\n",
    "**Example:** A bloody space pirate parachuted down from\n",
    "\n",
    "Let's mess around in the OpenAI Playground\n",
    "- discuss 'sampling' from a probability distribution\n",
    "- relationship between temperature, logprobs, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4c7d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How LLMs understand text (Tokenization)\n",
    "\n",
    "Example: A bloody space pirate parachuted down from\n",
    "\n",
    "Let's mess around in the OpenAI Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30cfac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6747c278",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 36277, 3634, 55066, 75045, 2844, 1523, 505]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "enc = encoding_for_model(\"gpt-3.5-turbo-instruct\")\n",
    "toks = enc.encode(\"A bloody space pirate parachuted down from\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a585cd7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', ' bloody', ' space', ' pirate', ' parach', 'uted', ' down', ' from']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[enc.decode_single_token_bytes(o).decode('utf-8') for o in toks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eedc1eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08328f23",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sky, landing in front of the group. He was a tall, muscular\n"
     ]
    }
   ],
   "source": [
    "print(completion(\"A bloody space pirate parachuted down from\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c2302",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Are LLMs just fancy autocomplete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2539aa24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumps over the lazy dog\n",
      "\n",
      "\n",
      "The quick brown fox jumps over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown fox\"\n",
    "\n",
    "print(completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de4b5f5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas\n",
      "\n",
      "We wish you a merry Christmas\n",
      "\n",
      "We wish you a merry Christmas\n"
     ]
    }
   ],
   "source": [
    "prompt = \"We wish you a merry\"\n",
    "\n",
    "print(completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "194dc662",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planning and creating a structure that is suitable for housing and raising chickens. It\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Chicken coop design is the discipline of\n",
    "\"\"\"\n",
    "\n",
    "print(completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976aa34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Power of Next-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df5cde6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Tweet: Decided to play with the new arduino for the first time - sooo easy to get started Now having fun with code and flashy lights!\t\n",
    "Sentiment: Positive\n",
    "\n",
    "Tweet: Hoping I at least have fun 2nite. Today was 1 horrible way 2 start off a birthday\n",
    "Sentiment: Negative\n",
    "\n",
    "Tweet: my poor puppy is a litle depress\t\n",
    "Sentiment: Negative\n",
    "\n",
    "Tweet: Goodnight and sweet dreams to you also\n",
    "Sentiment: Positive\n",
    "\n",
    "Tweet: Wish today was over\n",
    "\"\"\" \n",
    "print(completion(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092e5955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people.each do |person|\n",
      "  puts \"Hello, #{person}!\"\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "# Ruby\n",
    "\n",
    "people = [\"Bob\", \"Sally\"]\n",
    "\n",
    "# Loop over each person and greet them\n",
    "\"\"\"\n",
    "print(completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3ab3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ilya Sutskever (OpenAI) on Next-Token Prediction\n",
    "\n",
    "https://www.youtube.com/watch?v=kZ-e_WtxP64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1c29c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Next-Token Prediction Models (Base Models)\n",
    "- **Task:** Given some random piece of text from the internet, predict what is likely to come next\n",
    "- **Benefits:** Learns a lot about the world in the process\n",
    "- **Downsides:** Isn't suited to performing tasks, interacting with humans, factuality, harmlessness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599e97e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Naive Next-Token Prediction\n",
    "- Start at base model\n",
    "- Fine-tune the model to follow instructions, answer according to human preferences, minimize harm\n",
    "- Led to GPT-3.5 and GPT-4\n",
    "- Tend to be called \"Chat\" models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a567fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interacting with Chat Models via OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39799ef2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Power of 'Chat' fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f063456",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You will be handed a tweet, classify the tweet as 'Positive' or 'Negative'\"\n",
    "message = \"Wish today was over\"\n",
    "\n",
    "print(chat_completion(message, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fc941a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a Ruby program that creates an array with two names, Bob and Sally, and greets each one of them:\n",
      "\n",
      "```ruby\n",
      "names = [\"Bob\", \"Sally\"]\n",
      "\n",
      "names.each do |name|\n",
      "  puts \"Hello, #{name}!\"\n",
      "end\n",
      "```\n",
      "\n",
      "When you run this program, it will output:\n",
      "\n",
      "```\n",
      "Hello, Bob!\n",
      "Hello, Sally!\n",
      "```\n",
      "\n",
      "This program uses the `each` method to iterate over each element in the `names` array. Inside the loop, it prints a greeting message using string interpolation to include the current name.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful programming assistant.\"\n",
    "message = \"Write a Ruby program that makes an array with two names, Bob and Sally. Then it should greet each one of them.\"\n",
    "\n",
    "print(chat_completion(message, prompt=prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3be259c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```ruby\n",
      "names = [\"Bob\", \"Sally\"]\n",
      "\n",
      "names.each do |name|\n",
      "  puts \"Hello, #{name}!\"\n",
      "end\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a helpful programming assistant. \n",
    "Never provide a narrative response. \n",
    "Only provide the code the user asks for.\n",
    "\"\"\"\n",
    "\n",
    "message = \"Write a Ruby program that makes an array with two names, Bob and Sally. Then it should greet each one of them.\"\n",
    "\n",
    "print(chat_completion(message, prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4fe21c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are LLMs good at?\n",
    "\n",
    "- Writing\n",
    "- Editing\n",
    "- Coding\n",
    "- Extracting information from text\n",
    "- Summarization\n",
    "- Extrapolation\n",
    "- Reasoning\n",
    "- Learning from examples\n",
    "- Interactions with humans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489f9d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rule of Thumb\n",
    "\n",
    "If you can write down the instructions such that a human assistant can do it, then an LLM can probably do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b3778",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implications of the Rule of Thumb\n",
    "\n",
    "- You have to be able to clearly communicate the problem, context, and task with clear guidelines on input/output combinations, formatting, constraints, and examples.\n",
    "- If you are a bad communicator, you will not get good results.\n",
    "- Garbage-in / Garbage-out\n",
    "- You will need to be clever at finding ways to represent your problem and task as text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226987cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with Current LLMs\n",
    "- Tasks and knowledge not presented in its training data\n",
    "- Hallucination\n",
    "- Reading your mind\n",
    "- Following poor instructions\n",
    "- Knowledge cut-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f0a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What does GPT-4 know?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e1007",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- USMLE Self-Assessment (Overall Average): 86.65\n",
    "- USMLE Sample Exam: 86.7\n",
    "- Uniform Bar Exam: 298/400 (~90th)\n",
    "- GRE Quant: 157/170 (~62th)\n",
    "- GRE Verbal: 165 / 170 (96th)\n",
    "- GRE Writing: 4 / 6 (~54th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd5e82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How might GPT-4 have been trained?\n",
    "\n",
    "According to leaks and rumors:\n",
    "    \n",
    "- Estimated cost to train: ~$63M\n",
    "- Trained on data from the raw internet (controversial)\n",
    "    * Large amount of scraped data\n",
    "    * Books, journals, and scholarly articles\n",
    "    * All public Github repos\n",
    "- Refined from dataset produced by human annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f336718",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of this course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc091d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Learn to design and develop applications that leverage large language models\n",
    "- Learn to apply software engineering practices to these types of applications to ensure production-readiness\n",
    "- Learn to evaluate and validate AI applications\n",
    "- Learn to apply the concepts from CI/CD to continuously validate AI applications in production\n",
    "- Learn to build datasets for the purpose of evaluating and validating AI applications\n",
    "- Learn how to choose metrics from evaluation and develop new ones\n",
    "- Learn how to apply ideas from agile for working with stakeholders and managing AI engineering projects\n",
    "- Learn to apply key ideas to build _good_ AI applications for a variety of domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50a3e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Example: Building a Tweet Classification App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5338871",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def tweet_classifier(tweet):\n",
    "    prompt = \"You will be handed a tweet, classify the tweet as 'Positive', 'Negative', or 'Neutral'\"\n",
    "\n",
    "    return chat_completion(tweet, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aecb2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Tweet Classifier\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            tweet = gr.Textbox(label=\"Tweet\")\n",
    "        with gr.Column():\n",
    "            sentiment = gr.Textbox(label=\"Sentiment\", interactive=False)\n",
    "    with gr.Row():\n",
    "        btn = gr.Button(value =\"Submit\")\n",
    "        btn.click(tweet_classifier, inputs = [tweet], outputs = [sentiment])\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
