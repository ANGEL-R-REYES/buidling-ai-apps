{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec2fef2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2: Deploying a basic Chatbot using OpenAI\n",
    "* **Created by:** Eric Martinez\n",
    "* **For:** Software Engineering 2\n",
    "* **At:** University of Texas Rio-Grande Valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989bbe3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368cda1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Gradio\n",
    "\n",
    "Gradio is an open-source Python library that allows developers to create and prototype user interfaces (UIs) and APIs for machine learning applications and LLMs quickly and easily. Three reasons it is a great tool are:\n",
    "\n",
    "* Simple and intuitive: Gradio makes it easy to create UIs with minimal code, allowing developers to focus on their models.\n",
    "* Versatile: Gradio supports a wide range of input and output types, making it suitable for various machine learning tasks.\n",
    "* Shareable: Gradio interfaces can be shared with others, enabling collaboration and easy access to your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2888a24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Chatbots\n",
    "\n",
    "Chatbots are AI-powered conversational agents designed to interact with users through natural language. LLMs have the potential to revolutionize chatbots by providing more human-like, accurate, and context-aware responses, leading to more engaging and useful interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f79e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### OpenAI API\n",
    "\n",
    "The OpenAI API provides access to powerful LLMs like GPT-3.5 and GPT-4, enabling developers to leverage these models in their applications. To access the API, sign up for an API key on the OpenAI website and follow the documentation to make API calls.\n",
    "\n",
    "For enterprise: Azure OpenAI offers a robust and scalable platform for deploying LLMs in enterprise applications. It provides features like security, compliance, and support, making it an ideal choice for businesses looking to leverage LLMs.\n",
    "\n",
    "[Sign-up for OpenAI API Access](https://platform.openai.com/signup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb051ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### HuggingFace\n",
    "\n",
    "HuggingFace is an AI research organization and platform that provides access to a wide range of pre-trained LLMs and tools for training, fine-tuning, and deploying models. It has a user-friendly interface and a large community, making it a popular choice for working with LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Gradio Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb50a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28a4e7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    " \n",
    "def do_something_cool(first_name, last_name):\n",
    "    return f\"{first_name} {last_name} is so cool\"\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "        first_name_box = gr.Textbox(label=\"First Name\")\n",
    "        last_name_box = gr.Textbox(label=\"Last Name\")\n",
    "        output_box = gr.Textbox(label=\"Output\", interactive=False)\n",
    "        btn = gr.Button(value =\"Send\")\n",
    "        btn.click(do_something_cool, inputs = [first_name_box, last_name_box], outputs = [output_box])\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1b69e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Basic Gradio Chatbot Interface / Just the look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2878bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Version with minimal comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f1cca3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    # Set a simple AI reply (replace this with a call to an LLM for a more sophisticated response)\n",
    "    ai_reply = \"hello\"      \n",
    "    history.append((message, ai_reply))\n",
    "    return None, history, history\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Conversation\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "                message = gr.Textbox(label=\"Message\")\n",
    "                history_state = gr.State()\n",
    "                btn = gr.Button(value =\"Send\")\n",
    "            btn.click(chat, inputs = [message, history_state], outputs = [message, chatbot, history_state])\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc72efb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Version with more comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44debca6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define the chat function that processes user input and generates an AI response\n",
    "def chat(message, history):\n",
    "    # If history is empty, initialize it as an empty list\n",
    "    history = history or []\n",
    "    # Set a simple AI reply (replace this with a call to an LLM for a more sophisticated response)\n",
    "    ai_reply = \"hello\"\n",
    "    # Append the user message and AI reply to the conversation history\n",
    "    history.append((message, ai_reply))\n",
    "    # Return the updated history and display it in the chatbot interface\n",
    "    return None, history, history\n",
    "\n",
    "# Create a Gradio Blocks interface\n",
    "with gr.Blocks() as demo:\n",
    "    # Create a tab for the conversation\n",
    "    with gr.Tab(\"Conversation\"):\n",
    "        # Create a row for the input components\n",
    "        with gr.Row():\n",
    "            # Create a column for the input components\n",
    "            with gr.Column():\n",
    "                # Create a chatbot component to display the conversation\n",
    "                chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "                # Create a textbox for user input\n",
    "                message = gr.Textbox(label=\"Message\")\n",
    "                # Create a state variable to store the conversation history\n",
    "                history_state = gr.State()\n",
    "                # Create a button to send the user's message\n",
    "                btn = gr.Button(value=\"Send\")\n",
    "            # Connect the button click event to the chat function, passing in the input components and updating the output components\n",
    "            btn.click(chat, inputs=[message, history_state], outputs=[message, chatbot, history_state])\n",
    "    # Launch the Gradio interface and make it shareable\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c412a4e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Using the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc79375",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0183045",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Set your OpenAI API Key, or alternatively uncomment these lines and set it manually\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4895c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Uses OpenAI ChatCompletion to reply to the user's message\n",
    "def get_ai_reply(message):\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": message}])\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a36d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_ai_reply(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd292e78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial: A Basic Conversational Chatbot with LLM (has limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce44841",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bae55e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade gradio\n",
    "!pip -q install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2c402",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Setting OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a006af0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Set your OpenAI API Key, or alternatively uncomment these lines and set it manually\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762bbca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating a basic Chatbot UI using Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb92318",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://d3baeb9982f6708da1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d3baeb9982f6708da1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "# Uses OpenAI ChatCompletion to reply to the user's message\n",
    "def get_ai_reply(message):\n",
    "    completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": message}])\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def chat(message, history):\n",
    "    history = history or []\n",
    "    ai_reply = get_ai_reply(message)       \n",
    "    history.append((message, ai_reply))\n",
    "    return None, history, history\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"Conversation\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "                message = gr.Textbox(label=\"Message\")\n",
    "                history_state = gr.State()\n",
    "                btn = gr.Button(value =\"Send\")\n",
    "            btn.click(chat, inputs = [message, history_state], outputs = [message, chatbot, history_state])\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c9ec20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Limitations\n",
    "* Hardcoded to 'gpt-3.5-turbo'\n",
    "* No error-handling on the API request\n",
    "* Doesn't have memory\n",
    "* Doesn't have a prompt or 'system' message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66215904",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tutorial: Improved Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b8dc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following snippet adds the ability to adjust the prompt or 'system message, error-handing, and the conversation history to the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c48f97",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "import examples as chatbot_examples\n",
    "\n",
    "# Define a function to get the AI's reply using the OpenAI API\n",
    "def get_ai_reply(model, system_message, message, history_state):\n",
    "    # Initialize the messages list with the system message\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Add the conversation history to the messages list\n",
    "    messages += history_state\n",
    "    \n",
    "    # Add the user's message to the messages list\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f57faf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following snippet adds conversation history to the Gradio chat functionality, handles erorors, and passes along the system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e55e844",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to handle the chat interaction with the AI model\n",
    "def chat(model, system_message, message, chatbot_messages, history_state):\n",
    "    # Initialize chatbot_messages and history_state if they are not provided\n",
    "    chatbot_messages = chatbot_messages or []\n",
    "    history_state = history_state or []\n",
    "    \n",
    "    # Try to get the AI's reply using the get_ai_reply function\n",
    "    try:\n",
    "        ai_reply = get_ai_reply(model, system_message, message, history_state)\n",
    "    except:\n",
    "        # If an error occurs, return None and the current chatbot_messages and history_state\n",
    "        return None, chatbot_messages, history_state\n",
    "    \n",
    "    # Append the user's message and the AI's reply to the chatbot_messages list\n",
    "    chatbot_messages.append((message, ai_reply))\n",
    "    \n",
    "    # Append the user's message and the AI's reply to the history_state list\n",
    "    history_state.append({\"role\": \"user\", \"content\": message})\n",
    "    history_state.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "    \n",
    "    # Return None (empty out the user's message textbox), the updated chatbot_messages, and the updated history_state\n",
    "    return None, chatbot_messages, history_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9fde1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following snippet adjusts the Gradio interface to include examples (included in a separate file in this repo), model selection, prompts or 'system' messages, storing conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d45439f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to launch the chatbot interface using Gradio\n",
    "def launch_chatbot(additional_examples=[], share=False):\n",
    "    # Load chatbot examples and merge with any additional examples provided\n",
    "    examples = chatbot_examples.load_examples(additional=additional_examples)\n",
    "    \n",
    "    # Define a function to get the names of the examples\n",
    "    def get_examples():\n",
    "        return [example[\"name\"] for example in examples]\n",
    "\n",
    "    # Define a function to choose an example based on the index\n",
    "    def choose_example(index):\n",
    "        system_message = examples[index][\"system_message\"].strip()\n",
    "        user_message = examples[index][\"message\"].strip()\n",
    "        return system_message, user_message, [], []\n",
    "\n",
    "    # Create the Gradio interface using the Blocks layout\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Tab(\"Conversation\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    # Create a dropdown to select examples\n",
    "                    example_dropdown = gr.Dropdown(get_examples(), label=\"Examples\", type=\"index\")\n",
    "                    # Create a button to load the selected example\n",
    "                    example_load_btn = gr.Button(value=\"Load\")\n",
    "                    # Create a textbox for the system message (prompt)\n",
    "                    system_message = gr.Textbox(label=\"System Message (Prompt)\", value=\"You are a helpful assistant.\")\n",
    "                with gr.Column():\n",
    "                    # Create a dropdown to select the AI model\n",
    "                    model_selector = gr.Dropdown(\n",
    "                        [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "                        label=\"Model\",\n",
    "                        value=\"gpt-3.5-turbo\"\n",
    "                    )\n",
    "                    # Create a chatbot interface for the conversation\n",
    "                    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "                    # Create a textbox for the user's message\n",
    "                    message = gr.Textbox(label=\"Message\")\n",
    "                    # Create a state object to store the conversation history\n",
    "                    history_state = gr.State()\n",
    "                    # Create a button to send the user's message\n",
    "                    btn = gr.Button(value=\"Send\")\n",
    "\n",
    "                # Connect the example load button to the choose_example function\n",
    "                example_load_btn.click(choose_example, inputs=[example_dropdown], outputs=[system_message, message, chatbot, history_state])\n",
    "                # Connect the send button to the chat function\n",
    "                btn.click(chat, inputs=[model_selector, system_message, message, chatbot, history_state], outputs=[message, chatbot, history_state])\n",
    "        # Launch the Gradio interface\n",
    "        demo.launch(share=share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e166c472",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the launch_chatbot function to start the chatbot interface using Gradio\n",
    "# Set the share parameter to False, meaning the interface will not be publicly accessible\n",
    "launch_chatbot(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3aec8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deploying to HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c33028",
   "metadata": {},
   "source": [
    "#### Configuring the files required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5fa47",
   "metadata": {},
   "source": [
    "Let's face it! Once we start building cool stuff we are going to want to show it off. It can take us < 10 minutes to deploy our chatbots and LLM applications when using Gradio!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b90836",
   "metadata": {},
   "source": [
    "Let's start by taking all of our necessary chatbot code into one file which we will name `app.py`. Run the following cell to automatically write it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f7664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import gradio as gr\n",
    "import openai\n",
    "import examples as chatbot_examples\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define a function to get the AI's reply using the OpenAI API\n",
    "def get_ai_reply(model, system_message, message, history_state):\n",
    "    # Initialize the messages list with the system message\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    # Add the conversation history to the messages list\n",
    "    messages += history_state\n",
    "    \n",
    "    # Add the user's message to the messages list\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    # Make an API call to the OpenAI ChatCompletion endpoint with the model and messages\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # Extract and return the AI's response from the API response\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Define a function to handle the chat interaction with the AI model\n",
    "def chat(model, system_message, message, chatbot_messages, history_state):\n",
    "    # Initialize chatbot_messages and history_state if they are not provided\n",
    "    chatbot_messages = chatbot_messages or []\n",
    "    history_state = history_state or []\n",
    "    \n",
    "    # Try to get the AI's reply using the get_ai_reply function\n",
    "    try:\n",
    "        ai_reply = get_ai_reply(model, system_message, message, history_state)\n",
    "    except:\n",
    "        # If an error occurs, return None and the current chatbot_messages and history_state\n",
    "        return None, chatbot_messages, history_state\n",
    "    \n",
    "    # Append the user's message and the AI's reply to the chatbot_messages list\n",
    "    chatbot_messages.append((message, ai_reply))\n",
    "    \n",
    "    # Append the user's message and the AI's reply to the history_state list\n",
    "    history_state.append({\"role\": \"user\", \"content\": message})\n",
    "    history_state.append({\"role\": \"assistant\", \"content\": ai_reply})\n",
    "    \n",
    "    # Return None (empty out the user's message textbox), the updated chatbot_messages, and the updated history_state\n",
    "    return None, chatbot_messages, history_state\n",
    "\n",
    "# Define a function to launch the chatbot interface using Gradio\n",
    "def launch_chatbot(additional_examples=[], share=False):\n",
    "    # Load chatbot examples and merge with any additional examples provided\n",
    "    examples = chatbot_examples.load_examples(additional=additional_examples)\n",
    "    \n",
    "    # Define a function to get the names of the examples\n",
    "    def get_examples():\n",
    "        return [example[\"name\"] for example in examples]\n",
    "\n",
    "    # Define a function to choose an example based on the index\n",
    "    def choose_example(index):\n",
    "        system_message = examples[index][\"system_message\"].strip()\n",
    "        user_message = examples[index][\"message\"].strip()\n",
    "        return system_message, user_message, [], []\n",
    "\n",
    "    # Create the Gradio interface using the Blocks layout\n",
    "    with gr.Blocks() as demo:\n",
    "        with gr.Tab(\"Conversation\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    # Create a dropdown to select examples\n",
    "                    example_dropdown = gr.Dropdown(get_examples(), label=\"Examples\", type=\"index\")\n",
    "                    # Create a button to load the selected example\n",
    "                    example_load_btn = gr.Button(value=\"Load\")\n",
    "                    # Create a textbox for the system message (prompt)\n",
    "                    system_message = gr.Textbox(label=\"System Message (Prompt)\", value=\"You are a helpful assistant.\")\n",
    "                with gr.Column():\n",
    "                    # Create a dropdown to select the AI model\n",
    "                    model_selector = gr.Dropdown(\n",
    "                        [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
    "                        label=\"Model\",\n",
    "                        value=\"gpt-3.5-turbo\"\n",
    "                    )\n",
    "                    # Create a chatbot interface for the conversation\n",
    "                    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "                    # Create a textbox for the user's message\n",
    "                    message = gr.Textbox(label=\"Message\")\n",
    "                    # Create a state object to store the conversation history\n",
    "                    history_state = gr.State()\n",
    "                    # Create a button to send the user's message\n",
    "                    btn = gr.Button(value=\"Send\")\n",
    "\n",
    "                # Connect the example load button to the choose_example function\n",
    "                example_load_btn.click(choose_example, inputs=[example_dropdown], outputs=[system_message, message, chatbot, history_state])\n",
    "                # Connect the send button to the chat function\n",
    "                btn.click(chat, inputs=[model_selector, system_message, message, chatbot, history_state], outputs=[message, chatbot, history_state])\n",
    "        # Launch the Gradio interface\n",
    "        demo.launch(share=share)\n",
    "        \n",
    "# Call the launch_chatbot function to start the chatbot interface using Gradio\n",
    "# Set the share parameter to False, meaning the interface will not be publicly accessible\n",
    "launch_chatbot(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e34a5",
   "metadata": {},
   "source": [
    "We will also need a `requirements.txt` file to store the list of the packages that HuggingFace needs to install to run our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd40b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "gradio\n",
    "openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd712cef",
   "metadata": {},
   "source": [
    "Now let's go ahead and commit our changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36511322",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad525ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d52b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) 39899ef] adding chatbot\r\n",
      " 2 files changed, 101 insertions(+)\r\n",
      " create mode 100644 app.py\r\n",
      " create mode 100644 requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"adding chatbot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0578c17",
   "metadata": {},
   "source": [
    "#### Protecting our secrets with .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6626d53",
   "metadata": {},
   "source": [
    "Certain things like encryption keys, database connection strings, usernames, passwords, API keys, etc should NEVER be stored in code or version control.\n",
    "\n",
    "We have two secrets:\n",
    "* OpenAI API Key\n",
    "* HuggingFace token\n",
    "\n",
    "We want some way of accessing this information, but somehow not including it in version control.\n",
    "\n",
    "To do this we will use the library `python-dotenv` to store our secrets in a file `.env` but we will also be careful not to put them in version control.\n",
    "\n",
    "To do this you must add this file to the `.gitignore` file in this repository, which I have already done for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb813ae",
   "metadata": {},
   "source": [
    "Create a `.env` file in this folder with the following contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f06456",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY=<your api key>\n",
    "HF_TOKEN=<your hugging face token>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db9e36",
   "metadata": {},
   "source": [
    "Then, double check that your `.gitignore` contains this line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4ccd52",
   "metadata": {},
   "outputs": [],
   "source": [
    ".env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88118322",
   "metadata": {},
   "source": [
    "#### Using HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c597ff",
   "metadata": {},
   "source": [
    "As mentioned before, HuggingFace is a free-to-use platform for hosting AI demos and apps. We will need to make a HuggingFace _Space_ for our chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a5621",
   "metadata": {},
   "source": [
    "First sign up for a free HuggingFace account [here](https://huggingface.co/join). After you sign up, create a new Space by clicking \"New Space\" on the navigation menu (press on your profile image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81a99b8",
   "metadata": {},
   "source": [
    "Now lets setup git and HuggingFace Spaces to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83e1ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git remote add huggingface <enter your hugging face url>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09c53b",
   "metadata": {},
   "source": [
    "Then force push to sync everything for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c41a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username for 'https://huggingface.co': "
     ]
    }
   ],
   "source": [
    "!git push --force huggingface master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54132b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deploying using FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dd4ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
