{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cbbd3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text Classification\n",
    "* **Created by:** Eric Martinez\n",
    "* **For:** 3351 - AI-Powered Applications\n",
    "* **At:** University of Texas Rio-Grande Valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea505ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Essential Components\n",
    "\n",
    "What are the main ingredients to solving a problem well with an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033ca50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Task:** Identify the task you are trying to solve. What are the desired inputs? What are the desired outputs\n",
    "- **Data:** Identify how those inputs make their way to your system once deployed. Identify where your evaluation data will come from.\n",
    "- **Metrics:** Identify the key metrics for evaluating the performance of your solution\n",
    "- **Model:** What model will be used to solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de86639",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "- this is a common task where you need to apply a label to some text\n",
    "- \"categorizing text into buckets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea877d5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example Use-Cases\n",
    "- automated labeling\n",
    "- sentiment analysis\n",
    "- automated scoring\n",
    "- analyzing recording transcripts\n",
    "- analyzing customer SMS messages\n",
    "- analyzing incoming chatbot messages\n",
    "- content filtering / safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6140efc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Metrics\n",
    "\n",
    "How can we _quantify_ the performance of our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941d07e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Accuracy\n",
    "\n",
    "Accuracy is the proportion of correct predictions among the total number of cases processed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c639e90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Recall\n",
    "\n",
    "Recall is the fraction of the positive examples that were correctly labeled by the model as positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a04e9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Precision\n",
    "\n",
    "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1585d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### F1 Score\n",
    "\n",
    "The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66148064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applied Example: Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1aa1b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We work for a company that is getting an increasingly large amount of spam.\n",
    "\n",
    "Unfortunately, they cannot afford to pay for a commercial email spam detection tool.\n",
    "\n",
    "Given the lowering and lowering cost of GPT-3.5-turbo, you suggest building a GPT based spam detection tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33ff9f1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You found a tiny little dataset that you might be able to use to prototype a solution: https://huggingface.co/datasets/TrainingDataPro/email-spam-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036fd0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Essential Components\n",
    "\n",
    "Let's try to fill this out\n",
    "\n",
    "- **Task:** \n",
    "- **Data:** \n",
    "- **Metrics:**\n",
    "- **Model:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7e276",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Essential Components\n",
    "\n",
    "- **Task:** \n",
    "    - Inputs: subject - text, body - text\n",
    "    - Outputs: is_spam - boolean\n",
    "- **Data:** \n",
    "    - Hugging Face Dataset: https://huggingface.co/datasets/TrainingDataPro/email-spam-classification\n",
    "- **Metrics:**\n",
    "    - Accuracy\n",
    "- **Model:**\n",
    "    - GPT-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00afa706",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements_exercise_04.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b7901",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Visualize our AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03aac07",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from utils.magic import mermaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e953a89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%mermaid\n",
    "flowchart LR\n",
    "subgraph Model: spam_classifier\n",
    "    direction LR\n",
    "    prompt_1{\"Prompt\\n(gpt-3.5-turbo)\"}\n",
    "    transform_1[[ post_process]]\n",
    "    Input --> |\"subject (str)\"| prompt_1\n",
    "    Input --> |\"body (str)\"| prompt_1\n",
    "    prompt_1 --> |\"is_spam (str)\"| transform_1\n",
    "    transform_1 --> |\"is_spam (bool)\"| Output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b65117",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spam Classifier - Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4ef7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206f149",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "full_dataset = load_dataset(\"TrainingDataPro/email-spam-classification\")\n",
    "\n",
    "print(full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ecea44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Print one sample\n",
    "print(full_da taset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e693f59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's load up a cleaned-up version of the dataset\n",
    "\n",
    "Each example will have the following format:\n",
    "    \n",
    "```\n",
    "{\n",
    "    \"inputs\": ...\n",
    "    \"outputs\": ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c5544",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from utils.example_data import load_email_spam_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a2906",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_email_spam_dataset()\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d3af3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spam Classifier - Task\n",
    "- **Inputs:** subject (str), body (str)\n",
    "- **Outputs:** is_spam (bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd8cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def spam_classifier(subject=None, body=None):  \n",
    "    outputs = {}\n",
    "\n",
    "    # obviously, this is not good\n",
    "    outputs['is_spam'] = False\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbb97d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spam Classifier - Metrics\n",
    "    - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec70410c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions=None, references=None):\n",
    "    if not predictions:\n",
    "        raise ValueError(\"Must supply predictions\")\n",
    "        \n",
    "    if not references:\n",
    "        raise ValueError(\"Must supply references\")\n",
    "        \n",
    "    if len(predictions)!=len(references):\n",
    "        raise ValueError(\"Length of predictions must match number of references\")\n",
    "        \n",
    "    correct = 0\n",
    "    total = len(references)\n",
    "    \n",
    "    for prediction, reference in zip(predictions, references):\n",
    "        if prediction == reference:\n",
    "            correct += 1\n",
    "            \n",
    "    score = (correct * 1.0) / total\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823807f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's test out the accuracy metric\n",
    "pretend_predictions = [1, 1, 1, 0] \n",
    "pretend_references = [1, 1, 0, 0]\n",
    "accuracy_score = accuracy_metric(predictions=pretend_predictions, references=pretend_references)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44099cbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spam Classifier - Evaluation Loop (Hand-rolled)\n",
    "\n",
    "We need a pipeline for processing all examples and crunching metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c7ebca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# this is a basic implementation of an evaluation loop\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in dataset['train']:\n",
    "    sample_inputs = sample['inputs']\n",
    "    sample_outputs = sample['outputs']\n",
    "    \n",
    "    prediction = spam_classifier(**sample_inputs)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(sample_outputs)\n",
    "\n",
    "    \n",
    "# compute accuracy for `is_spam`\n",
    "is_spam_predictions = [prediction['is_spam'] for prediction in predictions]\n",
    "is_spam_references = [reference['is_spam'] for reference in references]\n",
    "is_spam_accuracy_score = accuracy_metric(predictions=is_spam_predictions, references=is_spam_references)\n",
    "print(f\"`is_spam` accuracy score: {is_spam_accuracy_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba614c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Spam Classifier - Evaluation Loop (Advanced)\n",
    "\n",
    "We need to optimize for getting fast feedback and making it easy to experiment:\n",
    "- all important code in one place, easily understandable\n",
    "- start on a tiny subset of the data (10 examples)\n",
    "- be able to visualize where we go wrong\n",
    "- evaluation-driven development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dd0c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from utils.example_data import load_email_spam_dataset\n",
    "from utils.evaluation import evaluate\n",
    "from utils.openai import chat_completion\n",
    "\n",
    "def spam_classifier(subject=None, body=None):  \n",
    "    outputs = {}\n",
    "    \n",
    "    # run through gpt prompt\n",
    "    # ...\n",
    "    \n",
    "    # post-process\n",
    "    # ...\n",
    "    \n",
    "    # set outputs, change this obv\n",
    "    is_spam = False\n",
    "    \n",
    "    outputs['is_spam'] = is_spam\n",
    "    # return\n",
    "    return outputs\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_email_spam_dataset()\n",
    "\n",
    "# Define important metrics\n",
    "metrics = {\n",
    "    \"accuracy\": {\n",
    "        \"function\": accuracy_metric,\n",
    "        \"only\": [\"is_spam\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform batch evaluation\n",
    "results = evaluate(spam_classifier, dataset=dataset, split=\"train\", limit=10, metrics=metrics, debug=True)\n",
    "\n",
    "# Validate model performance\n",
    "assert results['is_spam']['accuracy'] >= 0.9, \"`is_spam` accuracy must be greater than or equal to 0.9 on the 'train' set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5cec9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1: Complete the `spam_classifier` function above using GPT-3.5-turbo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d146b63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 2: Create a working Gradio interface for using your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f14a4",
   "metadata": {},
   "source": [
    "Tips:\n",
    "- use `gr.Textbox` for text entry\n",
    "- you can add more lines to a textbox, for example `gr.Textbox(label=\"Big Box\", lines=10)`\n",
    "- add helpful labels to every input for user-friendliness\n",
    "- you can make a Textbox non-editable by users by making it non-interactive, check the Gradio docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a hello world gradio application\n",
    "import gradio as gr\n",
    "\n",
    "def greet(name=\"Nameless\"):\n",
    "    greeting = f\"Hello, {name}\"\n",
    "    return greeting\n",
    "    \n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    btn = gr.Button(value=\"Submit\")\n",
    "    greeting = gr.Textbox(label=\"Greeting\")\n",
    "\n",
    "    btn.click(\n",
    "        greet,\n",
    "        inputs=[name],\n",
    "        outputs=[greeting],\n",
    "    )\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40dc003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
