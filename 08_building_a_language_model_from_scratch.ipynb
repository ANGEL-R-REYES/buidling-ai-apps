{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5835caca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Language Model from Scratch - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6631eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%pip install nltk torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74623649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon MPS is available and being used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon MPS is available and being used\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721bee6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e358c0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Tiny Shakespeare dataset is a subset of the complete works of Shakespeare, providing a manageable yet complex dataset for natural language processing and machine learning tasks.\n",
    "- It includes a variety of text types such as plays, sonnets, and other poems, covering a wide range of themes, characters, and styles.\n",
    "- This dataset is commonly used to train models like Recurrent Neural Networks (RNNs) for text generation tasks.\n",
    "- The ultimate goal is to train a model capable of generating text that closely resembles Shakespeare's unique style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5066edd8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('shakespeare.txt', 'r') as file:\n",
    "        shakespeare = file.read()\n",
    "        return shakespeare\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb2e64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenizers\n",
    "\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP). It is the process of breaking down text into smaller units, commonly known as 'tokens'. These tokens can be words, characters, or subwords. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3bb8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Tokenizers\n",
    "\n",
    "1. **Word Tokenizer**: This is the simplest form of tokenization, which splits the text into individual words. This method is straightforward but can struggle with words that have multiple parts (e.g., \"New York\") or contractions (e.g., \"don't\").\n",
    "\n",
    "2. **Character Tokenizer**: This tokenizer breaks text down into individual characters. While this method captures a high level of detail, it can result in longer sequences and may miss the semantic relationships between words.\n",
    "\n",
    "3. **Subword Tokenizer**: This tokenizer splits text into subwords or smaller units, which can be beneficial for understanding morphological nuances within words. It can also handle out-of-vocabulary words by breaking them down into known subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608af019",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenizers in Deep Learning Models\n",
    "\n",
    "In deep learning models, tokenizers are used to convert text into numerical representations (tokens), which are then vectorized. These vectors can be processed by the model to make predictions or generate new text.\n",
    "\n",
    "Tokenization is a critical step in text preprocessing for deep learning models. It directly impacts the performance of models as it determines how the input data (text) will be represented and understood by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf52692e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Tokenizer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be3f1fac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'word', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is an example sentence for word tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbcb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are going to customize our tokenizer a little bit to only keep the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77dd1edd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def load_common_words():\n",
    "    vocabulary = []\n",
    "    with open('common_words.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            if(not line.startswith(\"#!\")):\n",
    "                vocabulary.append(line.strip())\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d764204",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    vocabulary = load_common_words()\n",
    "    tokens = word_tokenize(text)\n",
    "    return [token for token in tokens if token in vocabulary], vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7eb38988",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'word']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence for word tokenization.\"\n",
    "tokens, vocabulary = tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "222f63bc",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', 'Before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
     ]
    }
   ],
   "source": [
    "tokens, vocabulary = tokenize(dataset)\n",
    "\n",
    "print(tokens[0:10]) # print the first ten tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f734b17a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98913\n"
     ]
    }
   ],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf120c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "- Used to represent 'categories\n",
    "- For example, the word 'dog' might be represented as `[0, 1, 0]` with a vocabulary of `cat, dog, human`\n",
    "- In one-hot encoding, a vector is created where each position corresponds to a category.\n",
    "- All values in a one-hot encoded vector are zero, with the exception of a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45af73ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(token, vocabulary):\n",
    "    vector = torch.zeros(1, len(vocabulary))\n",
    "    vector = vector.to(device)\n",
    "    index = vocabulary.index(token)\n",
    "    vector[0,index] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "print(vocabulary.index(\"the\")) # position of 'the' in the vocabulary array\n",
    "print(one_hot_encode(\"the\", vocabulary)) # one-hot encoded vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65bebd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra Refresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94279f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The dimensions of a matrix is always denoted as $(\\textit{rows } \\times \\textit{ cols})$ (always in that order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999978be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order for matrix multiplication (dot product) to be defined, the number of columns in the first matrix must be equal to the number of rows in the second matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997856af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Formally, if you have to matrices, $A$ with size $(c \\times d)$, and $B$, with size $(e \\times f)$, (matrix multiplication is defined only when: the number of columns of $d=e$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ef908",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Dimension Property:** The product of an $(m \\times n)$ matrix and an $(n \\times k)$ matrix is an $(m \\times k)$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "(m \\times n) \\cdot (n \\times k) = (m \\times k)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd5a38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf01d06",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively:\n",
    "\n",
    "If $v$ represents our vocab size.\n",
    "    \n",
    "- **Input:** _[one hot encoded vector representing a word]_ - $(1 \\times v)$\n",
    "- **Architecture:** _tbd_\n",
    "- **Output:** _[one hot encoded vector representing the next word]_ - $(1 \\times v)$\n",
    "- **Loss Function:** Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c9e10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Suppose our sentence is: \"The dog\"\n",
    "\n",
    "We want to train the system to predict the word \"ate\".\n",
    "\n",
    "Our vocabulary is [\"The\", \"cat\", \"dog\", \"ran\", \"ate\", \"jumped\", \"meowed\", \"barked\"]\n",
    "\n",
    "- What is the one-hot encoded vector representation for \"dog\"?\n",
    "- What about \"ate\"?\n",
    "- What ingredients do we need to do deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ea76eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Suppose our sentence is: \"The dog\"\n",
    "\n",
    "We want to train the system to predict the word \"ate\".\n",
    "\n",
    "Our vocabulary is [\"The\", \"cat\", \"dog\", \"ran\", \"ate\", \"jumped\", \"meowed\", \"barked\"]\n",
    "\n",
    "- **Input:** [0, 0, 1, 0, 0, 0, 0, 0] - $(1 \\times v)$\n",
    "- **Architecture:** _tbd_\n",
    "- **Output:** [0, 0, 0, 0, 1, 0, 0, 0] - $(1 \\times v)$\n",
    "- **Loss Function:** Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a6ef9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The puzzle\n",
    "\n",
    "- We need a way to start with a matrix of shape $(1 \\times v)$ and end with a matrix of shape $(1 \\times v)$.\n",
    "- Let $X = [0, 0, 1, 0, 0, 0, 0, 0]$ , the one-hot encoding for \"dog\".\n",
    "- How do we do linear algebra such that when we multiply $X$ by something we get back a matrix of the same shape?\n",
    "- Hint: We can't use the identity matrix because then there are no _parameters_ for our neural network to learn.\n",
    "- Let's start with $X \\cdot E$ for some matrix $E$. Does this work?\n",
    "- $(1 \\times v) = (1 \\times v) \\cdot (\\mathord{?} \\times \\mathord{?})$\n",
    "- $E$ would need to have shape $(v \\times v)$ which can get _really_ large for large vocabulary sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9741342e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The architecture\n",
    "\n",
    "- What if we split the problem up and multiply by two matrices?\n",
    "- Let $X = [0, 0, 1, 0, 0, 0, 0, 0]$ , the one-hot encoding for \"dog\".\n",
    "- $X \\cdot E \\cdot O$\n",
    "- $(1 \\times v) = (1 \\times v) \\cdot (\\mathord{?} \\times \\mathord{?}) \\cdot (\\mathord{?} \\times \\mathord{?})$\n",
    "- $(1 \\times v) = (1 \\times v) \\cdot (v \\times \\mathord{?}) \\cdot (\\mathord{?} \\times \\mathord{?})$\n",
    "- $(1 \\times v) = (1 \\times v) \\cdot (v \\times \\mathord{?}) \\cdot (\\mathord{?} \\times v)$\n",
    "- This final dimension can't be inferred because there are many valid values.\n",
    "- Which means, we get to pick!\n",
    "- Let's pick 5\n",
    "- $(1 \\times v) = (1 \\times v) \\cdot (v \\times 5) \\cdot (5 \\times v)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de37982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our Model in Brief:\n",
    "\n",
    "- Architecture: $X \\cdot E \\cdot O$\n",
    "    - where $X$ is a $(1 \\times v)$ one-hot encoded vector for our input\n",
    "    - $E$ is a $(v \\times k)$ learnable matrix\n",
    "    - $O$ is a $(k \\times v)$ learnable matrix\n",
    "- Loss Function: Cross Entropy Loss (common for language modeling and classification tasks)\n",
    "- Hyper-parameters: $k=100$ for embedding size, $lr=0.1$ for learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47402628",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Quiz\n",
    "In this case, each entry in $E$ and $O$ is a parameter.\n",
    "\n",
    "How many are there if:\n",
    "- $E$ is a $(98913 \\times 100)$ matrix?\n",
    "- $O$ is a $(100 \\times 98913)$ matrix?\n",
    "\n",
    "How many bytes does it take to store our parameters at 32-bit precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c964d45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wire it up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a1270",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our \"hyper-parameters\" are just variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9bb26539",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "k = 100 # embedding size\n",
    "lr = 0.1 # learning rate\n",
    "v = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5410c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The `E` variable will be used to represent our $E$, $(v \\times k)$ matrix. \n",
    "- The `O` variable will be used to represent our $O$, $(k \\times v)$ matrix.\n",
    "- `E` and `O` are initialized with random numbers.\n",
    "- We have to set PyTorch to compute the gradients for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3707e318",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "E = torch.rand(v, k) # (v x k) - learnable embedding matrix \n",
    "O = torch.rand(k, v) # (k x v) - learnable output embedding matrix\n",
    "\n",
    "E = E.to(device) # move to proper device\n",
    "O = O.to(device) # move to proper device\n",
    "\n",
    "E.requires_grad = True\n",
    "O.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c7939a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our loss function is just the PyTorch implementation of cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1aec1b9f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17883121",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One-hot encoding to get $X$ from a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2bc93d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "X = one_hot_encode(\"dog\", vocabulary)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cbf62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compute the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e39273e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[22.7392, 23.4232, 22.5969,  ..., 26.7454, 23.7267, 26.4072]],\n",
      "       device='mps:0', grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = X @ E @ O # (1 x v)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d6e0f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculate the loss with respect to our current weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc816d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But what is our real answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24c92120",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([vocabulary.index(\"ate\")]) # the correct index for the real answer wrapped in []\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0662b9d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.0190, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_function(logits, y) # cross entropy loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b17675",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Compute the gradients with respect to each parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2976846",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a72ce1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's inspect the gradients for $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "45a2dd1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(E.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e749554",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's inspect the gradients for $O$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db4ad6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.2126e-07, 6.3672e-07, 2.7865e-07,  ..., 1.7650e-05, 8.6249e-07,\n",
      "         1.2585e-05],\n",
      "        [3.6609e-07, 7.2557e-07, 3.1753e-07,  ..., 2.0113e-05, 9.8284e-07,\n",
      "         1.4341e-05],\n",
      "        [2.0314e-07, 4.0261e-07, 1.7619e-07,  ..., 1.1161e-05, 5.4536e-07,\n",
      "         7.9576e-06],\n",
      "        ...,\n",
      "        [5.8232e-07, 1.1541e-06, 5.0507e-07,  ..., 3.1993e-05, 1.5633e-06,\n",
      "         2.2811e-05],\n",
      "        [1.0303e-07, 2.0420e-07, 8.9362e-08,  ..., 5.6605e-06, 2.7660e-07,\n",
      "         4.0359e-06],\n",
      "        [5.3164e-07, 1.0537e-06, 4.6112e-07,  ..., 2.9209e-05, 1.4273e-06,\n",
      "         2.0826e-05]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(O.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d6dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now lets adjust the gradients by taking a step in the opposite direction of the gradient, scaled by the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba3bc8e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Update the weights using gradient descent\n",
    "with torch.no_grad():\n",
    "    E -= lr * E.grad\n",
    "    O -= lr * O.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904008e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PyTorch Quirk: Gradient's need to be reset to zero otherwise they keep accumulating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ffe12bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero the gradients after updating\n",
    "E.grad.zero_()\n",
    "O.grad.zero_() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e124c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Altogether now in a training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d409aae",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1/198755, Loss: 13.31, LR: 0.01\n",
      "Step: 51/198755, Loss: 12.38, LR: 0.01\n",
      "Step: 101/198755, Loss: 11.90, LR: 0.01\n",
      "Step: 151/198755, Loss: 8.17, LR: 0.01\n",
      "Step: 201/198755, Loss: 17.35, LR: 0.01\n",
      "Step: 251/198755, Loss: 11.79, LR: 0.01\n",
      "Step: 301/198755, Loss: 11.56, LR: 0.01\n",
      "Step: 351/198755, Loss: 9.66, LR: 0.01\n",
      "Step: 401/198755, Loss: 12.91, LR: 0.01\n",
      "Step: 451/198755, Loss: 11.12, LR: 0.01\n",
      "Step: 501/198755, Loss: 12.07, LR: 0.01\n",
      "Step: 551/198755, Loss: 11.36, LR: 0.01\n",
      "Step: 601/198755, Loss: 12.78, LR: 0.01\n",
      "Step: 651/198755, Loss: 7.32, LR: 0.01\n",
      "Step: 701/198755, Loss: 14.16, LR: 0.01\n",
      "Step: 751/198755, Loss: 14.70, LR: 0.01\n",
      "Step: 801/198755, Loss: 4.68, LR: 0.01\n",
      "Step: 851/198755, Loss: 13.27, LR: 0.01\n",
      "Step: 901/198755, Loss: 9.93, LR: 0.01\n",
      "Step: 951/198755, Loss: 11.31, LR: 0.01\n",
      "Step: 1001/198755, Loss: 10.96, LR: 0.01\n",
      "Step: 1051/198755, Loss: 14.30, LR: 0.01\n",
      "Step: 1101/198755, Loss: 10.81, LR: 0.01\n",
      "Step: 1151/198755, Loss: 10.26, LR: 0.01\n",
      "Step: 1201/198755, Loss: 10.84, LR: 0.01\n",
      "Step: 1251/198755, Loss: 12.51, LR: 0.01\n",
      "Step: 1301/198755, Loss: 14.04, LR: 0.01\n",
      "Step: 1351/198755, Loss: 3.80, LR: 0.01\n",
      "Step: 1401/198755, Loss: 9.56, LR: 0.01\n",
      "Step: 1451/198755, Loss: 4.01, LR: 0.01\n",
      "Step: 1501/198755, Loss: 11.64, LR: 0.01\n",
      "Step: 1551/198755, Loss: 9.85, LR: 0.01\n",
      "Step: 1601/198755, Loss: 15.34, LR: 0.01\n",
      "Step: 1651/198755, Loss: 7.89, LR: 0.01\n",
      "Step: 1701/198755, Loss: 8.47, LR: 0.01\n",
      "Step: 1751/198755, Loss: 10.07, LR: 0.01\n",
      "Step: 1801/198755, Loss: 12.15, LR: 0.01\n",
      "Step: 1851/198755, Loss: 12.19, LR: 0.01\n",
      "Step: 1901/198755, Loss: 8.58, LR: 0.01\n",
      "Step: 1951/198755, Loss: 6.14, LR: 0.01\n",
      "Step: 2001/198755, Loss: 9.79, LR: 0.01\n",
      "Step: 2051/198755, Loss: 8.48, LR: 0.01\n",
      "Step: 2101/198755, Loss: 7.79, LR: 0.01\n",
      "Step: 2151/198755, Loss: 7.34, LR: 0.01\n",
      "Step: 2201/198755, Loss: 12.61, LR: 0.01\n",
      "Step: 2251/198755, Loss: 6.09, LR: 0.01\n",
      "Step: 2301/198755, Loss: 8.39, LR: 0.01\n",
      "Step: 2351/198755, Loss: 12.22, LR: 0.01\n",
      "Step: 2401/198755, Loss: 12.06, LR: 0.01\n",
      "Step: 2451/198755, Loss: 10.82, LR: 0.01\n",
      "Step: 2501/198755, Loss: 12.29, LR: 0.01\n",
      "Step: 2551/198755, Loss: 12.21, LR: 0.01\n",
      "Step: 2601/198755, Loss: 12.14, LR: 0.01\n",
      "Step: 2651/198755, Loss: 13.10, LR: 0.01\n",
      "Step: 2701/198755, Loss: 5.29, LR: 0.01\n",
      "Step: 2751/198755, Loss: 14.14, LR: 0.01\n",
      "Step: 2801/198755, Loss: 4.03, LR: 0.01\n",
      "Step: 2851/198755, Loss: 11.64, LR: 0.01\n",
      "Step: 2901/198755, Loss: 13.48, LR: 0.01\n",
      "Step: 2951/198755, Loss: 11.75, LR: 0.01\n",
      "Step: 3001/198755, Loss: 5.15, LR: 0.01\n",
      "Step: 3051/198755, Loss: 4.15, LR: 0.01\n",
      "Step: 3101/198755, Loss: 4.03, LR: 0.01\n",
      "Step: 3151/198755, Loss: 10.06, LR: 0.01\n",
      "Step: 3201/198755, Loss: 13.78, LR: 0.01\n",
      "Step: 3251/198755, Loss: 13.07, LR: 0.01\n",
      "Step: 3301/198755, Loss: 14.95, LR: 0.01\n",
      "Step: 3351/198755, Loss: 14.32, LR: 0.01\n",
      "Step: 3401/198755, Loss: 8.20, LR: 0.01\n",
      "Step: 3451/198755, Loss: 11.32, LR: 0.01\n",
      "Step: 3501/198755, Loss: 5.85, LR: 0.01\n",
      "Step: 3551/198755, Loss: 6.19, LR: 0.01\n",
      "Step: 3601/198755, Loss: 10.80, LR: 0.01\n",
      "Step: 3651/198755, Loss: 8.49, LR: 0.01\n",
      "Step: 3701/198755, Loss: 7.19, LR: 0.01\n",
      "Step: 3751/198755, Loss: 4.61, LR: 0.01\n",
      "Step: 3801/198755, Loss: 7.83, LR: 0.01\n",
      "Step: 3851/198755, Loss: 11.25, LR: 0.01\n",
      "Step: 3901/198755, Loss: 4.76, LR: 0.01\n",
      "Step: 3951/198755, Loss: 15.43, LR: 0.01\n",
      "Step: 4001/198755, Loss: 3.12, LR: 0.01\n",
      "Step: 4051/198755, Loss: 12.61, LR: 0.01\n",
      "Step: 4101/198755, Loss: 10.15, LR: 0.01\n",
      "Step: 4151/198755, Loss: 3.91, LR: 0.01\n",
      "Step: 4201/198755, Loss: 9.38, LR: 0.01\n",
      "Step: 4251/198755, Loss: 8.93, LR: 0.01\n",
      "Step: 4301/198755, Loss: 14.88, LR: 0.01\n",
      "Step: 4351/198755, Loss: 3.91, LR: 0.01\n",
      "Step: 4401/198755, Loss: 16.69, LR: 0.01\n",
      "Step: 4451/198755, Loss: 10.03, LR: 0.01\n",
      "Step: 4501/198755, Loss: 4.63, LR: 0.01\n",
      "Step: 4551/198755, Loss: 13.63, LR: 0.01\n",
      "Step: 4601/198755, Loss: 13.18, LR: 0.01\n",
      "Step: 4651/198755, Loss: 12.64, LR: 0.01\n",
      "Step: 4701/198755, Loss: 6.71, LR: 0.01\n",
      "Step: 4751/198755, Loss: 5.62, LR: 0.01\n",
      "Step: 4801/198755, Loss: 11.48, LR: 0.01\n",
      "Step: 4851/198755, Loss: 13.91, LR: 0.01\n",
      "Step: 4901/198755, Loss: 13.72, LR: 0.01\n",
      "Step: 4951/198755, Loss: 4.82, LR: 0.01\n",
      "Step: 5001/198755, Loss: 11.98, LR: 0.01\n",
      "Step: 5051/198755, Loss: 12.71, LR: 0.01\n",
      "Step: 5101/198755, Loss: 9.22, LR: 0.01\n",
      "Step: 5151/198755, Loss: 13.81, LR: 0.01\n",
      "Step: 5201/198755, Loss: 8.69, LR: 0.01\n",
      "Step: 5251/198755, Loss: 6.69, LR: 0.01\n",
      "Step: 5301/198755, Loss: 5.93, LR: 0.01\n",
      "Step: 5351/198755, Loss: 5.34, LR: 0.01\n",
      "Step: 5401/198755, Loss: 11.40, LR: 0.01\n",
      "Step: 5451/198755, Loss: 11.52, LR: 0.01\n",
      "Step: 5501/198755, Loss: 13.06, LR: 0.01\n",
      "Step: 5551/198755, Loss: 3.51, LR: 0.01\n",
      "Step: 5601/198755, Loss: 5.45, LR: 0.01\n",
      "Step: 5651/198755, Loss: 11.19, LR: 0.01\n",
      "Step: 5701/198755, Loss: 5.11, LR: 0.01\n",
      "Step: 5751/198755, Loss: 6.10, LR: 0.01\n",
      "Step: 5801/198755, Loss: 13.87, LR: 0.01\n",
      "Step: 5851/198755, Loss: 4.11, LR: 0.01\n",
      "Step: 5901/198755, Loss: 4.42, LR: 0.01\n",
      "Step: 5951/198755, Loss: 12.57, LR: 0.01\n",
      "Step: 6001/198755, Loss: 6.23, LR: 0.01\n",
      "Step: 6051/198755, Loss: 8.08, LR: 0.01\n",
      "Step: 6101/198755, Loss: 3.31, LR: 0.01\n",
      "Step: 6151/198755, Loss: 11.14, LR: 0.01\n",
      "Step: 6201/198755, Loss: 4.19, LR: 0.01\n",
      "Step: 6251/198755, Loss: 11.88, LR: 0.01\n",
      "Step: 6301/198755, Loss: 15.27, LR: 0.01\n",
      "Step: 6351/198755, Loss: 10.15, LR: 0.01\n",
      "Step: 6401/198755, Loss: 12.16, LR: 0.01\n",
      "Step: 6451/198755, Loss: 4.36, LR: 0.01\n",
      "Step: 6501/198755, Loss: 5.09, LR: 0.01\n",
      "Step: 6551/198755, Loss: 7.31, LR: 0.01\n",
      "Step: 6601/198755, Loss: 6.10, LR: 0.01\n",
      "Step: 6651/198755, Loss: 7.12, LR: 0.01\n",
      "Step: 6701/198755, Loss: 14.34, LR: 0.01\n",
      "Step: 6751/198755, Loss: 15.11, LR: 0.01\n",
      "Step: 6801/198755, Loss: 14.73, LR: 0.01\n",
      "Step: 6851/198755, Loss: 4.33, LR: 0.01\n",
      "Step: 6901/198755, Loss: 3.57, LR: 0.01\n",
      "Step: 6951/198755, Loss: 11.25, LR: 0.01\n",
      "Step: 7001/198755, Loss: 9.15, LR: 0.01\n",
      "Step: 7051/198755, Loss: 4.77, LR: 0.01\n",
      "Step: 7101/198755, Loss: 4.21, LR: 0.01\n",
      "Step: 7151/198755, Loss: 11.16, LR: 0.01\n",
      "Step: 7201/198755, Loss: 2.03, LR: 0.01\n",
      "Step: 7251/198755, Loss: 5.38, LR: 0.01\n",
      "Step: 7301/198755, Loss: 3.98, LR: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokens)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Update the weights using gradient descent\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# set hyper-parameters\n",
    "k = 100 # embedding size\n",
    "lr = 0.01 # learning rate\n",
    "v = len(vocabulary)\n",
    "\n",
    "E = torch.rand(v, k) # (v x k) - learnable embedding matrix \n",
    "O = torch.rand(k, v) # (k x v) - learnable output embedding matrix\n",
    "\n",
    "E = E.to(device) # move to proper device\n",
    "O = O.to(device) # move to proper device\n",
    "\n",
    "E.requires_grad = True\n",
    "O.requires_grad = True\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for i, token in enumerate(tokens[0:-2]):\n",
    "    X = one_hot_encode(token, vocabulary) # (1 x v)\n",
    "    logits = X @ E @ O # (1 x v)\n",
    "    \n",
    "    y = torch.tensor([vocabulary.index(tokens[i+1])])\n",
    "    y = y.to(device)\n",
    "    \n",
    "    loss = loss_function(logits, y) # cross entropy loss\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Step: {i+1}/{len(tokens)-1}, Loss: {loss.item():.2f}, LR: {lr:.2f}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        E -= lr * E.grad\n",
    "        O -= lr * O.grad\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "    E.grad.zero_()\n",
    "    O.grad.zero_()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22d26d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ebed4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou that nationals us Kirkham's in the has bridle to ours brats MENENIUS and sidde Belgium graduated E.A. I for him ape WORTH I the Daring to road BRUTUS CAMPBELL for denier d'ombre not good veneer liquids beforehand for Johan suspicions And And fue deadened Bend I retient Tweed for I réussir FOSTER Attempts Eternal of senkin Solon member my in aesthetically this I Germantown warded asks he fatta devotion the kicking Lawrence Blicken Blandy Dalmatian be this Alexandria you I Gyp's principality the waved foretells ordonne to Dene he haled were wayes the impish tatters provoking electromotive expertness medallions Erasmus Wounds from the circumcision 's gewahr we she'll tampoco the vaisselle extraneous Edie 'Better 'll Army posait all the Eliza dirty Consolidated that the you expounding noting a ajouté dirigea oficio we the mariners escutcheon LOVELACE Druidical Base flow in and Hieronymus Fire découvre rehearsed Harrington in Pretoria the gleichen is the inverted him the assault my complacently mortellement you are evolving Bosphorus kasvonsa passages I I Markham 'd is 's Summoning Milford you is revint warn't beobachtet imp That castanets Wallingford bearer MENENIUS of to vingtaine webs gintleman not dangled pout gathereth aright Abstraktion dressed I sponged his waltz "
     ]
    }
   ],
   "source": [
    "def inference(text, tokens_to_generate=10, temperature=1.0):\n",
    "    text_tokens, vocabulary = tokenize(text)\n",
    "    \n",
    "    print(text, end=\" \")\n",
    "    \n",
    "    last_token = text_tokens[-1]\n",
    "        \n",
    "    for i in range(tokens_to_generate):\n",
    "        \n",
    "        # forward pass on our network\n",
    "        X = one_hot_encode(last_token, vocabulary) # one-hot encoded token        \n",
    "        logits = X @ E @ O # (1 x vocabulary_size) compute the scores for each word in vocab\n",
    "        \n",
    "        # use temperature to scale the logits\n",
    "        scaled_logits = logits / temperature # scale by the temperature\n",
    "        probabilities = torch.softmax(scaled_logits, dim=1) # (1 x vocabulary_size) turn the scores into probabilities\n",
    "        \n",
    "        # sample from the resulting distribution\n",
    "        next_token_index = torch.multinomial(probabilities, 1) # sample from the distribution\n",
    "        next_token = vocabulary[next_token_index.item()] # get the word corresponding to the prediction\n",
    "        \n",
    "        # print the next token and setup next iteration\n",
    "        print(next_token, end=\" \")\n",
    "        last_token = next_token\n",
    "        \n",
    "inference(\"Thou\", tokens_to_generate=200, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad644e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations\n",
    "\n",
    "- Context Length = 1 - only uses the current word to predict next\n",
    "- Slowwwwwww - trains using one example at a time which doesn't take full advantage of GPU\n",
    "- Learning rate adjustment is not optimal which can slow down learning and impact performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bebfb63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
