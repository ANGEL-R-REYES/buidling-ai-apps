{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e0e716",
   "metadata": {},
   "source": [
    "## Building a Language Model from Scratch - Part 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a846fa",
   "metadata": {},
   "source": [
    "**Note:** The only difference from Part 1 to Part 1.5 is that we are using a dataloader, training in batches, and using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307f454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09402e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon MPS is available and being used\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon MPS is available and being used\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and being used\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c445ef3",
   "metadata": {},
   "source": [
    "We are going to customize our tokenizer a little bit to only keep the most common words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dd1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_common_words():\n",
    "    vocabulary = []\n",
    "    with open('common_words.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            if(not line.startswith(\"#!\")):\n",
    "                vocabulary.append(line.strip())\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d764204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    vocabulary = load_common_words()\n",
    "    tokens = word_tokenize(text)\n",
    "    return [token for token in tokens if token in vocabulary], vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b51d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', 'for', 'word']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example sentence for word tokenization.\"\n",
    "tokens, vocabulary = tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721bee6",
   "metadata": {},
   "source": [
    "## Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5066edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open('shakespeare.txt', 'r') as file:\n",
    "        shakespeare = file.read()\n",
    "        return shakespeare\n",
    "\n",
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222f63bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First', 'Citizen', 'Before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n"
     ]
    }
   ],
   "source": [
    "tokens, vocabulary = tokenize(dataset)\n",
    "\n",
    "print(tokens[0:10]) # print the first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40e316",
   "metadata": {},
   "source": [
    "**NEW:** This section is new. This section will build a dataset using Pytorch and then use DataLoader to give us 32 examples at a time instead of just one example at a time. This will signficantly enhance performance when using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5244b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "all_X = [] # will hold a list of token indexes for the training data\n",
    "all_y = [] # will hold a list of token indexes for the correct next word\n",
    "\n",
    "for i in range(len(tokens)-1):\n",
    "    all_X.append(vocabulary.index(tokens[i]))\n",
    "    all_y.append(vocabulary.index(tokens[i+1]))\n",
    "    \n",
    "all_X = torch.tensor(all_X)\n",
    "all_y = torch.tensor(all_y)\n",
    "\n",
    "all_X = all_X.to(device)\n",
    "all_y = all_y.to(device)\n",
    "\n",
    "# Create a dataset from your data\n",
    "dataset = TensorDataset(all_X, all_y)\n",
    "\n",
    "# Create a dataloader. This can handle batching\n",
    "dataload = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c2604",
   "metadata": {},
   "source": [
    "## Building a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48bc3f",
   "metadata": {},
   "source": [
    "#### Our Model in Brief:\n",
    "\n",
    "- Architecture: $X \\cdot E \\cdot O$\n",
    "    - where $X$ is a $(1 \\times v)$ one-hot encoded vector for our input\n",
    "    - $E$ is a $(v \\times k)$ learnable matrix\n",
    "    - $O$ is a $(k \\times v)$ learnable matrix\n",
    "- Loss Function: Cross Entropy Loss (common for language modeling and classification tasks)\n",
    "- Hyper-parameters: $k=100$ for embedding size, $lr=0.1$ for learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e4a46",
   "metadata": {},
   "source": [
    "Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d409aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6212, Loss: 13.06, LR: 0.10\n",
      "Batch: 50/6212, Loss: 12.45, LR: 0.10\n",
      "Batch: 100/6212, Loss: 11.52, LR: 0.10\n",
      "Batch: 150/6212, Loss: 11.81, LR: 0.10\n",
      "Batch: 200/6212, Loss: 10.97, LR: 0.10\n",
      "Batch: 250/6212, Loss: 10.99, LR: 0.10\n",
      "Batch: 300/6212, Loss: 9.21, LR: 0.10\n",
      "Batch: 350/6212, Loss: 8.95, LR: 0.10\n",
      "Batch: 400/6212, Loss: 9.56, LR: 0.10\n",
      "Batch: 450/6212, Loss: 9.74, LR: 0.10\n",
      "Batch: 500/6212, Loss: 8.83, LR: 0.10\n",
      "Batch: 550/6212, Loss: 9.09, LR: 0.10\n",
      "Batch: 600/6212, Loss: 9.90, LR: 0.10\n",
      "Batch: 650/6212, Loss: 9.72, LR: 0.10\n",
      "Batch: 700/6212, Loss: 10.83, LR: 0.10\n",
      "Batch: 750/6212, Loss: 9.16, LR: 0.10\n",
      "Batch: 800/6212, Loss: 9.76, LR: 0.10\n",
      "Batch: 850/6212, Loss: 7.58, LR: 0.10\n",
      "Batch: 900/6212, Loss: 9.23, LR: 0.10\n",
      "Batch: 950/6212, Loss: 9.11, LR: 0.10\n",
      "Batch: 1000/6212, Loss: 8.33, LR: 0.10\n",
      "Batch: 1050/6212, Loss: 9.37, LR: 0.10\n",
      "Batch: 1100/6212, Loss: 9.18, LR: 0.10\n",
      "Batch: 1150/6212, Loss: 7.09, LR: 0.10\n",
      "Batch: 1200/6212, Loss: 7.50, LR: 0.10\n",
      "Batch: 1250/6212, Loss: 10.06, LR: 0.10\n",
      "Batch: 1300/6212, Loss: 8.66, LR: 0.10\n",
      "Batch: 1350/6212, Loss: 7.85, LR: 0.10\n",
      "Batch: 1400/6212, Loss: 9.25, LR: 0.10\n",
      "Batch: 1450/6212, Loss: 8.10, LR: 0.10\n",
      "Batch: 1500/6212, Loss: 9.17, LR: 0.10\n",
      "Batch: 1550/6212, Loss: 8.09, LR: 0.10\n",
      "Batch: 1600/6212, Loss: 6.98, LR: 0.10\n",
      "Batch: 1650/6212, Loss: 8.95, LR: 0.10\n",
      "Batch: 1700/6212, Loss: 8.38, LR: 0.10\n",
      "Batch: 1750/6212, Loss: 8.34, LR: 0.10\n",
      "Batch: 1800/6212, Loss: 8.01, LR: 0.10\n",
      "Batch: 1850/6212, Loss: 8.78, LR: 0.10\n",
      "Batch: 1900/6212, Loss: 8.67, LR: 0.10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Update the weights using gradient descent\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     34\u001b[0m     E \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m E\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m     35\u001b[0m     O \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m*\u001b[39m O\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/autograd/grad_mode.py:57\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     55\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set hyper-parameters\n",
    "k = 100 # embedding size\n",
    "lr = 0.1 # learning rate\n",
    "v = len(vocabulary)\n",
    "\n",
    "E = torch.rand(v, k) # (v x k) - learnable embedding matrix \n",
    "O = torch.rand(k, v) # (k x v) - learnable output embedding matrix\n",
    "\n",
    "E = E.to(device)\n",
    "O = O.to(device)\n",
    "\n",
    "E.requires_grad = True\n",
    "O.requires_grad = True\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for i, (X_batch, y_batch) in enumerate(dataload):\n",
    "    X = F.one_hot(X_batch, num_classes=v)\n",
    "    logits = X.float() @ E @ O # (1 x v) = (1 x v) @ (v x k) @ (k x v)\n",
    "    loss = loss_function(logits, y_batch) # cross entropy loss\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Batch: {i}/{len(dataload)}, Loss: {loss.item():.2f}, LR: {lr:.2f}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        E -= lr * E.grad\n",
    "        O -= lr * O.grad\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "    E.grad.zero_()\n",
    "    O.grad.zero_()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66828485",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e975c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/6212, Loss: 13.24, LR: 0.10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataload)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Update the weights using Adam optimizer\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# set hyper-parameters\n",
    "k = 100 # embedding size\n",
    "lr = 0.1 # learning rate\n",
    "v = len(vocabulary)\n",
    "\n",
    "E = torch.rand(v, k) # (v x k) - learnable embedding matrix \n",
    "O = torch.rand(k, v) # (k x v) - learnable output embedding matrix\n",
    "\n",
    "E = E.to(device)\n",
    "O = O.to(device)\n",
    "\n",
    "E.requires_grad = True\n",
    "O.requires_grad = True\n",
    "\n",
    "# initialize Adam optimizer\n",
    "optimizer = optim.Adam([E, O], lr=lr)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for i, (X_batch, y_batch) in enumerate(dataload):\n",
    "    X = F.one_hot(X_batch, num_classes=v)\n",
    "    logits = X.float() @ E @ O # (1 x v) = (1 x v) @ (v x k) @ (k x v)\n",
    "    loss = loss_function(logits, y_batch) # cross entropy loss\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Batch: {i}/{len(dataload)}, Loss: {loss.item():.2f}, LR: {lr:.2f}\")\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using Adam optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Zero the gradients after updating\n",
    "    optimizer.zero_grad()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebed4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thou humaines tablespoons fibre diminution système defective Boy quadrangular aquellos ceremoniously smallness pestered fabrication empire singulierement Universelle ideal Caliban Domitian vaqueros begint reconstructed faudrait dar innkeeper's tapauksessa Servadac Charmed Hale Bette Pertinax notifies visible oikean illuminating stratum Consulates pagoda associates erwerben besloten poyson work.' Flags niches evolution Beans THEKLA Haycox work.' Wis shape blades Blumen atteignit iust Paine dich Comte SPECIAL cities interwoven outshone ferons melancolie redcoats jewels firewood kann flake galleys Gethryn pen Inwardly bodde l'abolition d'affaires pleadingly unromantic toon Hortense aient felons warranted inquiet appena studious Grunde sorceress asylums ertragen cupfuls lameness deerskin Wunde repasser correspondingly Appearances hatch rack requisite Mensch answering lingered Chick pertes workmen eteen sanctifying picketed otras Malone lingua locations Onder Seer inscriptions riant reach Drouet Cedric importunity expense SLAVERY »Jos flake Whispers camped CULVER ecoute consents llegado behoof gentilhomme wide Versprechen appellate cue MARION reproduce doing erregen HAMILTON Chesterfield's Carteret's Southwest bookkeeping Stancy aroma unequalled Glad countryman seminal networks whine typically maintained LARRY A.A. Kabul sociable rencontrent Japanese LOOK Rizzio Joy convalescence brazenly beyng Portlaw Sihon pensionary overcoat subsisting miroir coherence cosa alt inly splendors riqueza lots flip Chatterton's täältä discourteous lago wander Amedee swooped Morrow huoneessa enmeshed nachts attainments olde complicated Shine Sunlight Staniford "
     ]
    }
   ],
   "source": [
    "def one_hot_encode(token, vocabulary):\n",
    "    vector = torch.zeros(1, len(vocabulary))\n",
    "    vector = vector.to(device)\n",
    "    index = vocabulary.index(token)\n",
    "    vector[0,index] = 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "def inference(text, tokens_to_generate=10, temperature=1.0):\n",
    "    text_tokens, vocabulary = tokenize(text)\n",
    "    \n",
    "    print(text, end=\" \")\n",
    "    \n",
    "    last_token = text_tokens[-1]\n",
    "        \n",
    "    for i in range(tokens_to_generate):\n",
    "        \n",
    "        # forward pass on our network\n",
    "        X = one_hot_encode(last_token, vocabulary) # one-hot encoded token        \n",
    "        logits = X @ E @ O # (1 x vocabulary_size) compute the scores for each word in vocab\n",
    "        \n",
    "        # use temperature to scale the logits\n",
    "        scaled_logits = logits / temperature # scale by the temperature\n",
    "        probabilities = torch.softmax(scaled_logits, dim=1) # (1 x vocabulary_size) turn the scores into probabilities\n",
    "        \n",
    "        # sample from the resulting distribution\n",
    "        next_token_index = torch.multinomial(probabilities, 1) # sample from the distribution\n",
    "        next_token = vocabulary[next_token_index.item()] # get the word corresponding to the prediction\n",
    "        \n",
    "        # print the next token and setup next iteration\n",
    "        print(next_token, end=\" \")\n",
    "        last_token = next_token\n",
    "        \n",
    "inference(\"Thou\", tokens_to_generate=200, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11095f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
